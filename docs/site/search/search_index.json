{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AutoRec Abstract Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.","title":"Home"},{"location":"#welcome-to-autorec","text":"","title":"Welcome to AutoRec"},{"location":"#abstract","text":"Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.","title":"Abstract"},{"location":"about/","text":"This package is developed by DATA LAB at Texas A&M University. Core Team Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#core-team","text":"Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"benchmark/","text":"","title":"Benchmark"},{"location":"install/","text":"Requirements Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.2.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup. Install AutoRec","title":"Installation"},{"location":"install/#requirements","text":"Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.2.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.","title":"Requirements"},{"location":"install/#install-autorec","text":"","title":"Install AutoRec"},{"location":"preprocessor/","text":"[source] BasePreprocessor autorecsys . pipeline . preprocessor . BasePreprocessor ( dataset_path = None , header = None , columns = None , delimiter = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Args: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. Attributes: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. data_df (DataFrame): Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. hash_sizes (list): Integer cardinality of categories in each categorical data column. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. test_percentage (float): Percentage for the test set. validate_percentage (float): Percentage for the validation set. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. [source] load_data BasePreprocessor . load_data ( ** kwargs ) Load data into Pandas DataFrame format. Args: **kwargs: Keyword arguments used to load dataset. [source] transform_categorical BasePreprocessor . transform_categorical () Transform categorical data. Note: Produce fit dictionary for categorical data. Obtain hash statistics from fit dictionary. Transform categorical data by using fit dictionary. [source] get_X BasePreprocessor . get_X () [source] get_y BasePreprocessor . get_y () [source] split_data BasePreprocessor . split_data ( X , y , test_percentage ) Split data into the train, validation, and test sets. Args: X (ndarray): (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray): (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float): Percentage of test set. Returns: X_train (ndarray): Matrix associated with the part of numerical and categorical data for model training. X_test (ndarray): Matrix associated with the part of numerical and categorical data for model testing. y_train (ndarray): Matrix associated with the part of label data used to train model. y_test (ndarray): Matrix associated with the part of label data for model testing. [source] preprocess BasePreprocessor . preprocess ( ** kwargs ) Apply all preprocess steps. Args: **kwargs: Keyword arguments used to preprocess dataset. [source] AvazuPreprocessor autorecsys . pipeline . preprocessor . AvazuPreprocessor ( dataset_path = \"/home/thwang1231/autorec/examples/datasets/avazu/sampled_train_10000.txt\" , header = 0 , columns = None , delimiter = \",\" , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/fit_dictionary.pkl\" , transform_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/transformed.pkl\" , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/train.pkl\" , validate_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/validate.pkl\" , test_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/test.pkl\" , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Args: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. test_percentage (float): Percentage for the test set. validate_percentage (float): Percentage for the validation set. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. [source] load_data AvazuPreprocessor . load_data () Load the relevant part of the Avazu dataset into Pandas DataFrame format. [source] preprocess AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns: X_train (ndarray): Training set of the numerical and categorical data. y_train (ndarray): Training set of the label data. X_validate (ndarray): Validation set of the numerical and categorical data. y_validate (ndarray): Validation set of the label data. X_test (ndarray): Testing set of the numerical and categorical data. y_test (ndarray): Testing set of the label data.","title":"Preprocessor"},{"location":"preprocessor/#basepreprocessor","text":"autorecsys . pipeline . preprocessor . BasePreprocessor ( dataset_path = None , header = None , columns = None , delimiter = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Args: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. Attributes: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. data_df (DataFrame): Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. hash_sizes (list): Integer cardinality of categories in each categorical data column. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. test_percentage (float): Percentage for the test set. validate_percentage (float): Percentage for the validation set. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. [source]","title":"BasePreprocessor"},{"location":"preprocessor/#load_data","text":"BasePreprocessor . load_data ( ** kwargs ) Load data into Pandas DataFrame format. Args: **kwargs: Keyword arguments used to load dataset. [source]","title":"load_data"},{"location":"preprocessor/#transform_categorical","text":"BasePreprocessor . transform_categorical () Transform categorical data. Note: Produce fit dictionary for categorical data. Obtain hash statistics from fit dictionary. Transform categorical data by using fit dictionary. [source]","title":"transform_categorical"},{"location":"preprocessor/#get_x","text":"BasePreprocessor . get_X () [source]","title":"get_X"},{"location":"preprocessor/#get_y","text":"BasePreprocessor . get_y () [source]","title":"get_y"},{"location":"preprocessor/#split_data","text":"BasePreprocessor . split_data ( X , y , test_percentage ) Split data into the train, validation, and test sets. Args: X (ndarray): (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray): (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float): Percentage of test set. Returns: X_train (ndarray): Matrix associated with the part of numerical and categorical data for model training. X_test (ndarray): Matrix associated with the part of numerical and categorical data for model testing. y_train (ndarray): Matrix associated with the part of label data used to train model. y_test (ndarray): Matrix associated with the part of label data for model testing. [source]","title":"split_data"},{"location":"preprocessor/#preprocess","text":"BasePreprocessor . preprocess ( ** kwargs ) Apply all preprocess steps. Args: **kwargs: Keyword arguments used to preprocess dataset. [source]","title":"preprocess"},{"location":"preprocessor/#avazupreprocessor","text":"autorecsys . pipeline . preprocessor . AvazuPreprocessor ( dataset_path = \"/home/thwang1231/autorec/examples/datasets/avazu/sampled_train_10000.txt\" , header = 0 , columns = None , delimiter = \",\" , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/fit_dictionary.pkl\" , transform_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/transformed.pkl\" , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/train.pkl\" , validate_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/validate.pkl\" , test_path = \"/home/thwang1231/autorec/examples/preprocessed/avazu/test.pkl\" , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Args: dataset_path (str): Path to load the dataset. header (int): Row number to use as column names. columns (list): String names associated with the columns of the dataset. delimiter (str): Separator used to parse lines. dtype_dict (dict): Map string column names to column data type. ignored_columns (list): String names associated with the columns to ignore. target_column (str): String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list): String names associated with the columns containing numerical data. categorical_columns (list): String names associated with the columns containing categorical data. categorical_filter (int): Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str): Path to the fit dictionary for categorical data. transform_path (str): Path to the transformed dataset. test_percentage (float): Percentage for the test set. validate_percentage (float): Percentage for the validation set. train_path (str): Path to the training set. validate_path (str): Path to the validation set. test_path (str): Path to the test set. [source]","title":"AvazuPreprocessor"},{"location":"preprocessor/#load_data_1","text":"AvazuPreprocessor . load_data () Load the relevant part of the Avazu dataset into Pandas DataFrame format. [source]","title":"load_data"},{"location":"preprocessor/#preprocess_1","text":"AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns: X_train (ndarray): Training set of the numerical and categorical data. y_train (ndarray): Training set of the label data. X_validate (ndarray): Validation set of the numerical and categorical data. y_validate (ndarray): Validation set of the label data. X_test (ndarray): Testing set of the numerical and categorical data. y_test (ndarray): Testing set of the label data.","title":"preprocess"},{"location":"examples/ctr_autoint/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset mini_criteo = np . load ( \"./examples/datasets/criteo/criteo_2M.npz\" ) # TODO: preprocess train val split train_X = [ mini_criteo [ 'X_int' ] . astype ( np . float32 ), mini_criteo [ 'X_cat' ] . astype ( np . float32 )] train_y = mini_criteo [ 'y' ] val_X , val_y = train_X , train_y # build the pipeline. dense_input_node = Input ( shape = [ 13 ]) sparse_input_node = Input ( shape = [ 26 ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = 13 , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = 26 , hash_size = [ 1444 , 555 , 175781 , 128509 , 306 , 19 , 11931 , 630 , 4 , 93146 , 5161 , 174835 , 3176 , 28 , 11255 , 165206 , 11 , 4606 , 2017 , 4 , 172322 , 18 , 16 , 56456 , 86 , 43356 ], embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr autoint"},{"location":"examples/ctr_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , HyperInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset mini_criteo = np . load ( \"./examples/datasets/criteo/criteo_2M.npz\" ) # TODO: preprocess train val split train_X = [ mini_criteo [ 'X_int' ] . astype ( np . float32 ), mini_criteo [ 'X_cat' ] . astype ( np . float32 )] train_y = mini_criteo [ 'y' ] val_X , val_y = train_X , train_y # build the pipeline. dense_input_node = Input ( shape = [ 13 ]) sparse_input_node = Input ( shape = [ 26 ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = 13 , embedding_dim = 16 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = 26 , hash_size = [ 1444 , 555 , 175781 , 128509 , 306 , 19 , 11931 , 630 , 4 , 93146 , 5161 , 174835 , 3176 , 28 , 11255 , 165206 , 11 , 4606 , 2017 , 4 , 172322 , 18 , 16 , 56456 , 86 , 43356 ], embedding_dim = 16 )( sparse_input_node ) sparse_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ]) dense_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ]) top_mlp_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_bottom_output , dense_feat_bottom_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr autorec"},{"location":"examples/ctr_benchmark/","text":"import argparse import time import os import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , DenseFeatureMapper , SparseFeatureMapper , \\ ElementwiseInteraction , FMInteraction , MLPInteraction , ConcatenateInteraction , \\ CrossNetInteraction , SelfAttentionInteraction , HyperInteraction , \\ PointWiseOptimizer from autorecsys.pipeline.preprocessor import MovielensCTRPreprocessor from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_dlrm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = MLPInteraction ( num_layers = 2 )( emb_list ) else : sparse_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] dense_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( sparse_feat_mlp_output + dense_feat_mlp_output ) return output def build_deepfm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ FMInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ FMInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_crossnet ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ CrossNetInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ CrossNetInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_autoint ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ SelfAttentionInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ SelfAttentionInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_neumf ( emb_dict ): emb_list = [ emb for _ , emb in emb_dict . items ()] innerproduct_output = [ ElementwiseInteraction ( elementwise_type = \"innerporduct\" )( emb_list )] mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = innerproduct_output + mlp_output return output def build_autorec ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = HyperInteraction ()( emb_list ) else : sparse_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ])] if 'sparse' in emb_dict else [] dense_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ])] if 'dense' in emb_dict else [] top_mlp_output = HyperInteraction ( meta_interator_num = 2 )( sparse_feat_bottom_output + dense_feat_bottom_output ) output = HyperInteraction ( meta_interator_num = 2 )([ top_mlp_output ]) return output if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' , default = 'dlrm' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' , default = \"avazu\" ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' , default = './examples/datasets/ml-1m/ratings.dat' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' , default = 'random' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' , default = 256 ) parser . add_argument ( '-trials' , type = int , help = 'try number' , default = 10 ) parser . add_argument ( '-gpu_index' , type = int , help = 'the index of gpu to use' , default = 0 ) args = parser . parse_args () print ( \"args:\" , args ) os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = str ( args . gpu_index ) if args . sep == None : args . sep = '::' # Load and preprocess dataset if args . data == \"avazu\" : avazu = AvazuPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocessing ( train_size = 0.8 , valid_size = 0.1 ) # dense_input_node = None sparse_input_node = Input ( shape = [ avazu . categ_num ]) input = [ sparse_input_node ] # dense_feat_emb = None sparse_feat_emb = SparseFeatureMapper ( num_of_fields = avazu . categ_num , hash_size = avazu . hash_sizes , embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'sparse' : sparse_feat_emb } if args . data == \"criteo\" : criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocessing ( train_size = 0.8 , valid_size = 0.1 ) # build the pipeline. dense_input_node = Input ( shape = [ criteo . numer_num ]) sparse_input_node = Input ( shape = [ criteo . categ_num ]) input = [ dense_input_node , sparse_input_node ] dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . numer_num , embedding_dim = 64 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . categ_num , hash_size = criteo . hash_sizes , embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'dense' : dense_feat_emb , 'sparse' : sparse_feat_emb } # select model if args . model == 'dlrm' : output = build_dlrm ( emb_dict ) if args . model == 'deepfm' : output = build_deepfm ( emb_dict ) if args . model == 'crossnet' : output = build_neumf ( emb_dict ) if args . model == 'autoint' : output = build_autorec ( emb_dict ) # if args.model == 'neumf': # output = build_autorec(emb_dict) if args . model == 'autorec' : output = build_autorec ( emb_dict ) output = PointWiseOptimizer ()( output ) model = CTRRecommender ( inputs = input , outputs = output ) # search and predict. searcher = Search ( model = model , tuner = args . search , ## hyperband, bayesian tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = args . batch_size , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) end_time = time . time () print ( \"runing time:\" , end_time - start_time ) print ( \"args\" , args ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X [: 10 ]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr benchmark"},{"location":"examples/ctr_crossnet/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , CrossNetInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset mini_criteo = np . load ( \"./examples/datasets/criteo/criteo_2M.npz\" ) # TODO: preprocess train val split train_X = [ mini_criteo [ 'X_int' ] . astype ( np . float32 ), mini_criteo [ 'X_cat' ] . astype ( np . float32 )] train_y = mini_criteo [ 'y' ] val_X , val_y = train_X , train_y # build the pipeline. dense_input_node = Input ( shape = [ 13 ]) sparse_input_node = Input ( shape = [ 26 ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = 13 , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = 26 , hash_size = [ 1444 , 555 , 175781 , 128509 , 306 , 19 , 11931 , 630 , 4 , 93146 , 5161 , 174835 , 3176 , 28 , 11255 , 165206 , 11 , 4606 , 2017 , 4 , 172322 , 18 , 16 , 56456 , 86 , 43356 ], embedding_dim = 2 )( sparse_input_node ) crossnet_output = CrossNetInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ crossnet_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr crossnet"},{"location":"examples/ctr_deepfm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset mini_criteo = np . load ( \"./examples/datasets/criteo/criteo_2M.npz\" ) # TODO: preprocess train val split train_X = [ mini_criteo [ 'X_int' ] . astype ( np . float32 ), mini_criteo [ 'X_cat' ] . astype ( np . float32 )] train_y = mini_criteo [ 'y' ] val_X , val_y = train_X , train_y # build the pipeline. dense_input_node = Input ( shape = [ 13 ]) sparse_input_node = Input ( shape = [ 26 ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = 13 , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = 26 , hash_size = [ 1444 , 555 , 175781 , 128509 , 306 , 19 , 11931 , 630 , 4 , 93146 , 5161 , 174835 , 3176 , 28 , 11255 , 165206 , 11 , 4606 , 2017 , 4 , 172322 , 18 , 16 , 56456 , 86 , 43356 ], embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr deepfm"},{"location":"examples/ctr_deepfm_test_avazu/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) avazu = AvazuPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocess () sparse_input_node = Input ( shape = [ len ( avazu . categorical_columns )]) # Step 1: Mapper sparse_feat_emb = SparseFeatureMapper ( num_of_fields = len ( avazu . categorical_columns ), hash_size = avazu . hash_sizes , embedding_dim = 2 )( sparse_input_node ) # Step 2.1: Interactor fm_output = FMInteraction ()([ sparse_feat_emb ]) # Step 2.2: Interactor top_mlp_output = MLPInteraction ()([ fm_output ]) # Step 3: Optimizer output = PointWiseOptimizer ()( top_mlp_output ) # Step 4: Recommender wrapper model = CTRRecommender ( inputs = [ sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr deepfm test avazu"},{"location":"examples/ctr_deepfm_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) set_seed ( 0 ) st = time . time () criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocessing ( train_size = 0.8 , valid_size = 0.1 ) print ( \"Preprocessing time: \\t \" , time . time () - st ) # build the pipeline. dense_input_node = Input ( shape = [ criteo . numer_num ]) sparse_input_node = Input ( shape = [ criteo . categ_num ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . numer_num , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . categ_num , hash_size = criteo . hash_sizes , embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) print ( \"Total time: \\t \" , time . time () - st ) # # load dataset # mini_criteo = np.load(\"./examples/datasets/criteo/criteo_2M.npz\") # # TODO: preprocess train val split # train_X = [mini_criteo['X_int'].astype(np.float32), mini_criteo['X_cat'].astype(np.float32)] # train_y = mini_criteo['y'] # val_X, val_y = train_X, train_y # # # build the pipeline. # dense_input_node = Input(shape=[13]) # sparse_input_node = Input(shape=[26]) # dense_feat_emb = DenseFeatureMapper( # num_of_fields=13, # embedding_dim=2)(dense_input_node) # # # TODO: preprocess data to get sparse hash_size # sparse_feat_emb = SparseFeatureMapper( # num_of_fields=26, # hash_size=[ # 1444, 555, 175781, 128509, 306, 19, # 11931, 630, 4, 93146, 5161, 174835, # 3176, 28, 11255, 165206, 11, 4606, # 2017, 4, 172322, 18, 16, 56456, # 86, 43356 # ], # embedding_dim=2)(sparse_input_node) # # fm_output = FMInteraction()([sparse_feat_emb]) # bottom_mlp_output = MLPInteraction()([dense_feat_emb]) # top_mlp_output = MLPInteraction()([fm_output, bottom_mlp_output]) # # output = PointWiseOptimizer()(top_mlp_output) # model = CTRRecommender(inputs=[dense_input_node, sparse_input_node], outputs=output) # # # AutoML search and predict. # searcher = Search(model=model, # tuner='random', # tuner_params={'max_trials': 2, 'overwrite': True}, # ) # searcher.search(x=train_X, # y=train_y, # x_val=val_X, # y_val=val_y, # objective='val_BinaryCrossentropy', # batch_size=10000 # ) # logger.info('First 10 Predicted Ratings: {}'.format(searcher.predict(x=val_X)[:10])) # logger.info('Predicting Accuracy (logloss): {}'.format(searcher.evaluate(x=val_X, y_true=val_y)))","title":"Ctr deepfm test criteo"},{"location":"examples/ctr_dlrm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset mini_criteo = np . load ( \"./examples/datasets/criteo/criteo_2M.npz\" ) # TODO: preprocess train val split train_X = [ mini_criteo [ 'X_int' ] . astype ( np . float32 ), mini_criteo [ 'X_cat' ] . astype ( np . float32 )] train_y = mini_criteo [ 'y' ] val_X , val_y = train_X , train_y # build the pipeline. dense_input_node = Input ( shape = [ 13 ]) sparse_input_node = Input ( shape = [ 26 ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = 13 , embedding_dim = 16 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = 26 , hash_size = [ 1444 , 555 , 175781 , 128509 , 306 , 19 , 11931 , 630 , 4 , 93146 , 5161 , 174835 , 3176 , 28 , 11255 , 165206 , 11 , 4606 , 2017 , 4 , 172322 , 18 , 16 , 56456 , 86 , 43356 ], embedding_dim = 16 )( sparse_input_node ) sparse_feat_mlp_output = MLPInteraction ()([ sparse_feat_emb ]) dense_feat_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ( num_layers = 2 )([ sparse_feat_mlp_output , dense_feat_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr dlrm"},{"location":"examples/ctr_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , PointWiseOptimizer , ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensCTRPreprocessor from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ml_1m = MovielensCTRPreprocessor ( \"./examples/datasets/ml-1m/ratings.dat\" ) ml_1m . preprocessing ( test_size = 0.1 , num_neg = 10 , mult = 2 ) train_X , train_y , val_X , val_y = ml_1m . train_X , ml_1m . train_y , ml_1m . val_X , ml_1m . val_y # build the pipeline. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = PointWiseOptimizer ()([ innerproduct_output , mlp_output ]) model = CTRRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 10 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 256 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X ))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr neumf"},{"location":"examples/ctr_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor st = time . time () # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo_path = \"./examples/datasets/criteo_full/train.txt\" criteo = CriteoPreprocessor ( criteo_path ) criteo . preprocessing ( test_size = 0.2 ) train_X , train_y , val_X , val_y = criteo . train_X , criteo . train_y , criteo . val_X , criteo . val_y # TODO: preprocess train val split # build the pipeline. dense_input_node = Input ( shape = [ criteo . numer_num ]) sparse_input_node = Input ( shape = [ criteo . categ_num ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . numer_num , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . categ_num , hash_size = criteo . hash_sizes , embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1000 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) print ( time . time () - st )","title":"Ctr test criteo"},{"location":"examples/rp_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"2\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset data = MovielensPreprocessor ( \"./examples/datasets/ml-1m/ratings.dat\" ) ##Movielens 10M Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-10M100K/ratings.dat\") ##Movielens latest Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-latest/ratings.csv\", sep=',') data . preprocessing ( val_test_size = 0.1 ) train_X , train_y = data . train_X , data . train_y val_X , val_y = data . val_X , data . val_y test_X , test_y = data . test_X , data . test_y user_num , item_num = data . user_num , data . item_num logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output1 = HyperInteraction ()([ user_emb , item_emb ]) output2 = HyperInteraction ()([ output1 , user_emb , item_emb ]) output3 = HyperInteraction ()([ output1 , output2 , user_emb , item_emb ]) output4 = HyperInteraction ()([ output1 , output2 , output3 , user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output4 ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , ## hyperband, bayesian tuner_params = { 'max_trials' : 100 , 'overwrite' : True },) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Rp autorec"},{"location":"examples/rp_benchmark/","text":"import argparse import time import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction , MLPInteraction , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_mf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_gmf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_mlp ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = user_num , embedding_dim = 64 )( input ) output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_neumf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) return model def build_autorec ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_1 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_1 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) user_emb_2 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_2 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = HyperInteraction ()([ user_emb_1 , item_emb_1 , user_emb_2 , item_emb_2 ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' ) parser . add_argument ( '-epochs' , type = int , help = 'epochs' ) parser . add_argument ( '-early_stop' , type = int , help = 'early stop' ) parser . add_argument ( '-trials' , type = int , help = 'try number' ) args = parser . parse_args () # print(\"args:\", args) if args . sep == None : args . sep = '::' # Load dataset if args . data == \"ml\" : data = MovielensPreprocessor ( args . data_path , sep = args . sep ) if args . data == \"netflix\" : dataset_paths = [ args . data_path + \"/combined_data_\" + str ( i ) + \".txt\" for i in range ( 1 , 5 )] data = NetflixPrizePreprocessor ( dataset_paths ) data . preprocessing ( val_test_size = 0.1 ) train_X , train_y = data . train_X , data . train_y val_X , val_y = data . val_X , data . val_y test_X , test_y = data . test_X , data . test_y user_num , item_num = data . user_num , data . item_num logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # select model if args . model == 'mf' : model = build_mf ( user_num , item_num ) if args . model == 'mlp' : model = build_mlp ( user_num , item_num ) if args . model == 'gmf' : model = build_gmf ( user_num , item_num ) if args . model == 'neumf' : model = build_neumf ( user_num , item_num ) if args . model == 'autorec' : model = build_autorec ( user_num , item_num ) # search and predict. searcher = Search ( model = model , tuner = args . search , ## hyperband, bayesian tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_mse' , batch_size = args . batch_size , epochs = args . epochs , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = args . early_stop )]) end_time = time . time () # print(\"Runing time:\", end_time - start_time) # print(\"Args\", args) logger . info ( 'Runing time: {} ' . format ( end_time - start_time )) logger . info ( 'Args: {} ' . format ( args )) logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = test_X ))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Rp benchmark"},{"location":"examples/rp_mf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import tensorflow as tf # gpus = tf.config.experimental.list_physical_devices(device_type='GPU') # for gpu in gpus: # tf.config.experimental.set_memory_growth(gpu, True) # import tensorflow as tf # physical_devices = tf.config.list_physical_devices('GPU') # tf.config.experimental.set_memory_growth(physical_devices[0], True) import logging from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset data = MovielensPreprocessor ( \"./examples/datasets/ml-1m/ratings.dat\" ) ##Movielens 10M Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-10M100K/ratings.dat\") ##Movielens latest Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-latest/ratings.csv\", sep=',') data . preprocessing ( val_test_size = 0.1 ) train_X , train_y = data . train_X , data . train_y val_X , val_y = data . val_X , data . val_y test_X , test_y = data . test_X , data . test_y user_num , item_num = data . user_num , data . item_num logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Rp mf"},{"location":"examples/rp_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , RatingPredictionOptimizer , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset data = MovielensPreprocessor ( \"./examples/datasets/ml-1m/ratings.dat\" ) ##Movielens 10M Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-10M100K/ratings.dat\") ##Movielens latest Dataset # data = MovielensPreprocessor(\"./examples/datasets/ml-latest/ratings.csv\", sep=',') data . preprocessing ( val_test_size = 0.1 ) train_X , train_y = data . train_X , data . train_y val_X , val_y = data . val_X , data . val_y test_X , test_y = data . test_X , data . test_y user_num , item_num = data . user_num , data . item_num logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # build the pipeline. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # random, greedy tuner_params = { \"max_trials\" : 5 , 'overwrite' : True } ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Rp neumf"}]}